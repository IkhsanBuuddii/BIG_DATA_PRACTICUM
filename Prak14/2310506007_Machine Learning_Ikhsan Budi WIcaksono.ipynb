{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e2a030e3",
      "metadata": {
        "id": "e2a030e3"
      },
      "source": [
        "# Hands-On Pertemuan 14: Advanced Machine Learning using Spark MLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "099562db",
      "metadata": {
        "id": "099562db"
      },
      "source": [
        "## Objectives:\n",
        "- Understand and implement advanced machine learning tasks using Spark MLlib.\n",
        "- Build and evaluate models using real-world datasets.\n",
        "- Explore techniques like feature engineering and hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77df771a",
      "metadata": {
        "id": "77df771a"
      },
      "source": [
        "## Introduction to Spark MLlib\n",
        "Spark MLlib is a scalable library for machine learning that integrates seamlessly with the Spark ecosystem. It supports a wide range of tasks, including regression, classification, clustering, and collaborative filtering."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ],
      "metadata": {
        "id": "OucMrrTQJkQE",
        "outputId": "2c2c78a8-4d65-440c-8ab6-b6716f87e792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OucMrrTQJkQE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9ae225b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9ae225b",
        "outputId": "695891fa-4f74-4ae3-8de2-354290917d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.9999999999999992]\n",
            "Intercept: 15.000000000000009\n"
          ]
        }
      ],
      "source": [
        "# Example: Linear Regression with Spark MLlib\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
        "\n",
        "# Load sample data\n",
        "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
        "columns = ['ID', 'Feature', 'Target']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Prepare data for modeling\n",
        "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Train a linear regression model\n",
        "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
        "model = lr.fit(df_transformed)\n",
        "\n",
        "# Print model coefficients\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b266267",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b266267",
        "outputId": "a6d0a287-2924-4e9a-d997-03e7224ccd12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [-12.262057929180484,4.087352266486688]\n",
            "Intercept: 11.56891272665312\n"
          ]
        }
      ],
      "source": [
        "# Practice: Logistic Regression\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Example dataset\n",
        "data = [(1, Vectors.dense([2.0, 3.0]), 0), (2, Vectors.dense([1.0, 5.0]), 1), (3, Vectors.dense([2.5, 4.5]), 1), (4, Vectors.dense([3.0, 6.0]), 0)]\n",
        "columns = ['ID', 'Features', 'Label']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Train logistic regression model\n",
        "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
        "model = lr.fit(df)\n",
        "\n",
        "# Display coefficients and summary\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9066e04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9066e04",
        "outputId": "e267e633-b495-4b5e-d198-daab01c72410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: [array([5.33333333, 5.33333333]), array([15., 15.])]\n"
          ]
        }
      ],
      "source": [
        "# Practice: KMeans Clustering\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Example dataset\n",
        "data = [(1, Vectors.dense([1.0, 1.0])), (2, Vectors.dense([5.0, 5.0])), (3, Vectors.dense([10.0, 10.0])), (4, Vectors.dense([15.0, 15.0]))]\n",
        "columns = ['ID', 'Features']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Train KMeans clustering model\n",
        "kmeans = KMeans(featuresCol='Features', k=2)\n",
        "model = kmeans.fit(df)\n",
        "\n",
        "# Show cluster centers\n",
        "centers = model.clusterCenters()\n",
        "print(f'Cluster Centers: {centers}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60a8d7e",
      "metadata": {
        "id": "a60a8d7e"
      },
      "source": [
        "## Homework\n",
        "- Load a real-world dataset into Spark and prepare it for machine learning tasks.\n",
        "- Build a classification model using Spark MLlib and evaluate its performance.\n",
        "- Explore hyperparameter tuning using cross-validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inisialisasi"
      ],
      "metadata": {
        "id": "GdQQknn532Hr"
      },
      "id": "GdQQknn532Hr"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5MwSAlS36DS",
        "outputId": "bbfc2dbe-062c-4e88-9283-e44d2eed68c8"
      },
      "id": "x5MwSAlS36DS",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Buat SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Bank Marketing Classification\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Path file\n",
        "file_path = \"/content/drive/My Drive/College/SMS3/BIGDATA/Prak14/bank-full.csv\"\n",
        "\n",
        "# Load dataset\n",
        "data = spark.read.csv(file_path, sep=\";\", header=True, inferSchema=True)\n",
        "\n",
        "# Tampilkan struktur dataset dan data sampel\n",
        "data.printSchema()\n",
        "data.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cakbbxg-5YAz",
        "outputId": "25a75529-2822-4868-9220-132c6557467e"
      },
      "id": "Cakbbxg-5YAz",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- y: string (nullable = true)\n",
            "\n",
            "+---+------------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "|age|         job|marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
            "+---+------------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "| 58|  management|married| tertiary|     no|   2143|    yes|  no|unknown|  5|  may|     261|       1|   -1|       0| unknown| no|\n",
            "| 44|  technician| single|secondary|     no|     29|    yes|  no|unknown|  5|  may|     151|       1|   -1|       0| unknown| no|\n",
            "| 33|entrepreneur|married|secondary|     no|      2|    yes| yes|unknown|  5|  may|      76|       1|   -1|       0| unknown| no|\n",
            "| 47| blue-collar|married|  unknown|     no|   1506|    yes|  no|unknown|  5|  may|      92|       1|   -1|       0| unknown| no|\n",
            "| 33|     unknown| single|  unknown|     no|      1|     no|  no|unknown|  5|  may|     198|       1|   -1|       0| unknown| no|\n",
            "+---+------------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# Tambahkan kolom label\n",
        "data = data.withColumn(\"label\", when(col(\"y\") == \"yes\", 1).otherwise(0))\n",
        "data = data.drop(\"y\")  # Hapus kolom y asli\n"
      ],
      "metadata": {
        "id": "SLWw3qcb6I27"
      },
      "id": "SLWw3qcb6I27",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lit, when\n",
        "\n",
        "# Tangani nilai null di kolom kategorikal dengan mengganti null menjadi \"unknown\"\n",
        "for col_name in categorical_cols:\n",
        "    data = data.withColumn(col_name, when(col(col_name).isNull(), lit(\"unknown\")).otherwise(col(col_name)))\n"
      ],
      "metadata": {
        "id": "6fLaB0c-68FT"
      },
      "id": "6fLaB0c-68FT",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tampilkan tipe data kolom\n",
        "data.printSchema()\n",
        "\n",
        "# Tampilkan nilai unik setiap kolom (opsional untuk debug)\n",
        "for col_name in categorical_cols:\n",
        "    data.select(col_name).distinct().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OBi-R016-RK",
        "outputId": "a0a4795d-2de2-4339-8981-5c6e4ce0f9c4"
      },
      "id": "-OBi-R016-RK",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- label: integer (nullable = false)\n",
            "\n",
            "+-------------+\n",
            "|          job|\n",
            "+-------------+\n",
            "|   management|\n",
            "|      retired|\n",
            "|      unknown|\n",
            "|self-employed|\n",
            "|      student|\n",
            "|  blue-collar|\n",
            "| entrepreneur|\n",
            "|       admin.|\n",
            "|   technician|\n",
            "|     services|\n",
            "|    housemaid|\n",
            "|   unemployed|\n",
            "+-------------+\n",
            "\n",
            "+--------+\n",
            "| marital|\n",
            "+--------+\n",
            "|divorced|\n",
            "| married|\n",
            "|  single|\n",
            "+--------+\n",
            "\n",
            "+---------+\n",
            "|education|\n",
            "+---------+\n",
            "|  unknown|\n",
            "| tertiary|\n",
            "|secondary|\n",
            "|  primary|\n",
            "+---------+\n",
            "\n",
            "+-------+\n",
            "|default|\n",
            "+-------+\n",
            "|     no|\n",
            "|    yes|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|housing|\n",
            "+-------+\n",
            "|     no|\n",
            "|    yes|\n",
            "+-------+\n",
            "\n",
            "+----+\n",
            "|loan|\n",
            "+----+\n",
            "|  no|\n",
            "| yes|\n",
            "+----+\n",
            "\n",
            "+---------+\n",
            "|  contact|\n",
            "+---------+\n",
            "|  unknown|\n",
            "| cellular|\n",
            "|telephone|\n",
            "+---------+\n",
            "\n",
            "+-----+\n",
            "|month|\n",
            "+-----+\n",
            "|  jun|\n",
            "|  aug|\n",
            "|  may|\n",
            "|  feb|\n",
            "|  sep|\n",
            "|  mar|\n",
            "|  oct|\n",
            "|  jul|\n",
            "|  nov|\n",
            "|  apr|\n",
            "|  dec|\n",
            "|  jan|\n",
            "+-----+\n",
            "\n",
            "+--------+\n",
            "|poutcome|\n",
            "+--------+\n",
            "| success|\n",
            "| unknown|\n",
            "|   other|\n",
            "| failure|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lit, when\n",
        "\n",
        "# Ganti null di kolom string dengan \"unknown\"\n",
        "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "\n",
        "for col_name in categorical_cols:\n",
        "    data = data.withColumn(col_name, when(col(col_name).isNull(), lit(\"unknown\")).otherwise(col(col_name)))\n"
      ],
      "metadata": {
        "id": "W0tDVmR87Ym7"
      },
      "id": "W0tDVmR87Ym7",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Buat StringIndexer dan OneHotEncoder\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid=\"skip\") for col in categorical_cols]\n",
        "encoders = [OneHotEncoder(inputCol=f\"{col}_indexed\", outputCol=f\"{col}_encoded\") for col in categorical_cols]\n",
        "\n",
        "# Tentukan kolom fitur\n",
        "feature_cols = [f\"{col}_encoded\" for col in categorical_cols] + \\\n",
        "               ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
        "\n",
        "# Gabungkan fitur\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Buat pipeline\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
        "\n",
        "# Fit dan transformasi data\n",
        "processed_data = pipeline.fit(data).transform(data)\n",
        "\n",
        "# Pilih kolom 'features' dan 'label'\n",
        "final_data = processed_data.select(\"features\", \"label\")\n",
        "final_data.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERBFrnXb6QM7",
        "outputId": "c2728c10-d00f-40df-b1fe-e5c10c796f3b"
      },
      "id": "ERBFrnXb6QM7",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|(42,[1,11,14,16,1...|    0|\n",
            "|(42,[2,12,13,16,1...|    0|\n",
            "|(42,[7,11,13,16,1...|    0|\n",
            "|(42,[0,11,16,17,1...|    0|\n",
            "|(42,[12,16,18,20,...|    0|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "rbcg6E2F75Ac"
      },
      "id": "rbcg6E2F75Ac",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Prediksi\n",
        "predictions = lr_model.transform(test_data)\n",
        "predictions.select(\"label\", \"prediction\", \"probability\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCOJ2qAC77jr",
        "outputId": "c210cb17-1830-4ab8-a836-fa12c520addd"
      },
      "id": "eCOJ2qAC77jr",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+--------------------+\n",
            "|label|prediction|         probability|\n",
            "+-----+----------+--------------------+\n",
            "|    0|       0.0|[0.95806866124047...|\n",
            "|    0|       0.0|[0.95034026133071...|\n",
            "|    0|       0.0|[0.93166535904926...|\n",
            "|    0|       0.0|[0.94675900602166...|\n",
            "|    0|       0.0|[0.92815044272412...|\n",
            "+-----+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"AUC Model: {auc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzQQsx-C8EPD",
        "outputId": "91f14174-d716-4cbf-daae-0af3a1d70b03"
      },
      "id": "YzQQsx-C8EPD",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Model: 0.6595512567719696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Grid parameter\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "# CrossValidator\n",
        "crossval = CrossValidator(estimator=lr,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=5)\n",
        "\n",
        "# Fit cross-validation model\n",
        "cv_model = crossval.fit(train_data)\n",
        "\n",
        "# Evaluasi model terbaik\n",
        "best_model = cv_model.bestModel\n",
        "best_predictions = best_model.transform(test_data)\n",
        "final_auc = evaluator.evaluate(best_predictions)\n",
        "print(f\"AUC Model Terbaik: {final_auc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx9Ovxhw8Iqr",
        "outputId": "4ca3dc84-01fc-4891-d562-14fd2f726e57"
      },
      "id": "wx9Ovxhw8Iqr",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Model Terbaik: 0.6314645273122867\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}